{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Define your neural network architecture\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(128, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01), input_shape=(input_dim,)),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(64, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01)),\n",
    "    tf.keras.layers.Dense(output_dim, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model with early stopping\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "history = model.fit(train_data, train_labels, epochs=100, validation_data=(val_data, val_labels), callbacks=[early_stopping])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X1 shape: (1000, 1)\n",
      "X2 shape: (1000, 1)\n",
      "X3 shape: (1000, 1)\n",
      "Y shape: (1000, 1)\n",
      "X shape (concatenated): (1000, 3)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Set a random seed for reproducibility\n",
    "np.random.seed(0)\n",
    "\n",
    "# Number of data points\n",
    "num_samples = 1000\n",
    "\n",
    "# Generate random values for X1, X2, and X3\n",
    "X1 = np.random.rand(num_samples, 1)  # Random values between 0 and 1\n",
    "X2 = np.random.randn(num_samples, 1)  # Random values from a standard normal distribution\n",
    "X3 = np.random.uniform(-2, 2, size=(num_samples, 1))  # Random values between -2 and 2\n",
    "\n",
    "# Generate labels (target values) using a simple rule\n",
    "# In this example, let's assume a simple linear relationship\n",
    "# You can adjust this relationship based on your specific problem\n",
    "# For logistic regression, labels should be binary (0 or 1)\n",
    "# Here, we'll use a threshold to create binary labels\n",
    "threshold = 0.5\n",
    "Y = ((2 * X1 + 0.5 * X2 + X3) > threshold).astype(int)\n",
    "\n",
    "# Display the shapes of the generated data\n",
    "print(\"X1 shape:\", X1.shape)\n",
    "print(\"X2 shape:\", X2.shape)\n",
    "print(\"X3 shape:\", X3.shape)\n",
    "print(\"Y shape:\", Y.shape)\n",
    "\n",
    "# Concatenate the input features into one matrix\n",
    "X = np.concatenate((X1, X2, X3), axis=1)\n",
    "\n",
    "# Display the shape of the concatenated input matrix\n",
    "print(\"X shape (concatenated):\", X.shape)\n",
    "\n",
    "# Split the dataset into training and testing sets (you can adjust the split ratio)\n",
    "split_ratio = 0.8\n",
    "split_index = int(num_samples * split_ratio)\n",
    "\n",
    "X_train = X[:split_index]\n",
    "Y_train = Y[:split_index]\n",
    "\n",
    "X_test = X[split_index:]\n",
    "Y_test = Y[split_index:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'target_value' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_10356\\2131987651.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;31m# Define your training data and labels\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mX1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# Shape: (3, 1)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m \u001b[0mY\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtarget_value\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# Shape: (1, 1)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;31m# Hyperparameters\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'target_value' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define the architecture\n",
    "input_dim = 3\n",
    "output_dim = 1\n",
    "\n",
    "# Initialize weights and biases\n",
    "np.random.seed(0)  # For reproducibility\n",
    "w = np.random.randn(output_dim, input_dim)\n",
    "b = np.zeros((output_dim, 1))\n",
    "\n",
    "# Define the sigmoid activation function\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Forward propagation\n",
    "def forward_propagation(X):\n",
    "    z = np.dot(w, X) + b\n",
    "    a = sigmoid(z)\n",
    "    return a\n",
    "\n",
    "# Backward propagation\n",
    "def backward_propagation(X, Y, a):\n",
    "    m = X.shape[1]\n",
    "    dz = a - Y\n",
    "    dw = (1 / m) * np.dot(dz, X.T)\n",
    "    db = (1 / m) * np.sum(dz)\n",
    "    \n",
    "    # Update weights and biases\n",
    "    w -= learning_rate * dw\n",
    "    b -= learning_rate * db\n",
    "\n",
    "# Define your training data and labels\n",
    "X = np.array([[X1, X2, X3]])  # Shape: (3, 1)\n",
    "Y = np.array([[target_value]])  # Shape: (1, 1)\n",
    "\n",
    "# Hyperparameters\n",
    "learning_rate = 0.01\n",
    "num_iterations = 10000\n",
    "\n",
    "# Training loop\n",
    "for i in range(num_iterations):\n",
    "    # Forward propagation\n",
    "    a = forward_propagation(X)\n",
    "    \n",
    "    # Compute cost (logistic loss)\n",
    "    cost = -np.mean(Y * np.log(a) + (1 - Y) * np.log(1 - a))\n",
    "    \n",
    "    # Backward propagation\n",
    "    backward_propagation(X, Y, a)\n",
    "    \n",
    "    # Print the cost every 1000 iterations\n",
    "    if i % 1000 == 0:\n",
    "        print(f\"Iteration {i}: Cost = {cost:.4f}\")\n",
    "\n",
    "# Final prediction\n",
    "final_prediction = forward_propagation(X)\n",
    "print(f\"Final prediction: {final_prediction[0][0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define the architecture\n",
    "input_dim = 3\n",
    "hidden_dim1 = 5\n",
    "hidden_dim2 = 5\n",
    "hidden_dim3 = 3\n",
    "output_dim = 1\n",
    "\n",
    "# Initialize weights and biases\n",
    "np.random.seed(0)  # For reproducibility\n",
    "w1 = np.random.randn(hidden_dim1, input_dim)\n",
    "b1 = np.zeros((hidden_dim1, 1))\n",
    "w2 = np.random.randn(hidden_dim2, hidden_dim1)\n",
    "b2 = np.zeros((hidden_dim2, 1))\n",
    "w3 = np.random.randn(hidden_dim3, hidden_dim2)\n",
    "b3 = np.zeros((hidden_dim3, 1))\n",
    "w4 = np.random.randn(output_dim, hidden_dim3)\n",
    "b4 = np.zeros((output_dim, 1))\n",
    "\n",
    "# Define the activation function (e.g., sigmoid)\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Forward propagation\n",
    "def forward_propagation(X):\n",
    "    z1 = np.dot(w1, X) + b1\n",
    "    a1 = sigmoid(z1)\n",
    "    z2 = np.dot(w2, a1) + b2\n",
    "    a2 = sigmoid(z2)\n",
    "    z3 = np.dot(w3, a2) + b3\n",
    "    a3 = sigmoid(z3)\n",
    "    z4 = np.dot(w4, a3) + b4\n",
    "    a4 = sigmoid(z4)\n",
    "    return a4\n",
    "\n",
    "# Backward propagation\n",
    "def backward_propagation(X, Y, cache):\n",
    "    m = X.shape[1]\n",
    "    \n",
    "    # Extract activations and caches\n",
    "    a1, a2, a3, a4 = cache\n",
    "    dz4 = a4 - Y\n",
    "    dw4 = (1 / m) * np.dot(dz4, a3.T)\n",
    "    db4 = (1 / m) * np.sum(dz4, axis=1, keepdims=True)\n",
    "    dz3 = np.dot(w4.T, dz4) * (a3 * (1 - a3))\n",
    "    dw3 = (1 / m) * np.dot(dz3, a2.T)\n",
    "    db3 = (1 / m) * np.sum(dz3, axis=1, keepdims=True)\n",
    "    dz2 = np.dot(w3.T, dz3) * (a2 * (1 - a2))\n",
    "    dw2 = (1 / m) * np.dot(dz2, a1.T)\n",
    "    db2 = (1 / m) * np.sum(dz2, axis=1, keepdims=True)\n",
    "    dz1 = np.dot(w2.T, dz2) * (a1 * (1 - a1))\n",
    "    dw1 = (1 / m) * np.dot(dz1, X.T)\n",
    "    db1 = (1 / m) * np.sum(dz1, axis=1, keepdims=True)\n",
    "    \n",
    "    # Update weights and biases\n",
    "    w4 -= learning_rate * dw4\n",
    "    b4 -= learning_rate * db4\n",
    "    w3 -= learning_rate * dw3\n",
    "    b3 -= learning_rate * db3\n",
    "    w2 -= learning_rate * dw2\n",
    "    b2 -= learning_rate * db2\n",
    "    w1 -= learning_rate * dw1\n",
    "    b1 -= learning_rate * db1\n",
    "\n",
    "# Define your training data and labels\n",
    "X = np.array([[X1, X2, X3]])  # Shape: (3, 1)\n",
    "Y = np.array([[target_value]])  # Shape: (1, 1)\n",
    "\n",
    "# Hyperparameters\n",
    "learning_rate = 0.01\n",
    "num_iterations = 10000\n",
    "\n",
    "# Training loop\n",
    "for i in range(num_iterations):\n",
    "    # Forward propagation\n",
    "    cache = forward_propagation(X)\n",
    "    \n",
    "    # Compute cost (e.g., mean squared error)\n",
    "    cost = np.mean(np.square(cache - Y))\n",
    "    \n",
    "    # Backward propagation\n",
    "    backward_propagation(X, Y, cache)\n",
    "    \n",
    "    # Print the cost every 1000 iterations\n",
    "    if i % 1000 == 0:\n",
    "        print(f\"Iteration {i}: Cost = {cost:.4f}\")\n",
    "\n",
    "# Final prediction\n",
    "final_prediction = forward_propagation(X_test)\n",
    "print(f\"Final prediction: {final_prediction[0][0]}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
